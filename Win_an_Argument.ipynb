{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "35KEU8wfGaB7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ✅Improve Retrieval - Scrape from pubmedisearch that already has a vector db with semantic search.\n",
        "# ✅LangGraph - Use it to select between what retriever to use, for PubMed, Arxiv,  Wikipedia or Google Search for finding latest scientifc research backed claims, known solutions, or definitions of stuff.\n",
        "# ✅Bias towards stance in question: Get only one stance from input, generate a prompt for both stances, get context and then decide answer."
      ],
      "metadata": {
        "id": "I4Wo-OmBi5tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installs"
      ],
      "metadata": {
        "id": "k37C3ZQsmpBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PH0Faty-LuB4",
        "outputId": "c7ae04df-115a-44b5-ec27-e730185b3532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.40)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_APV3rcBEsiR",
        "outputId": "5bee0b97-7c87-4c36-aa58-4bc80d04a0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-community arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWdCnd8KM-C_",
        "outputId": "8f7b88aa-833c-41bd-dfa5-40eacd53e33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain_community wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fR9F1F4kPHTt",
        "outputId": "c8103785-aad0-474f-b20c-d5b8bbe62b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain_community pubmed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tSKNxnUQeiM",
        "outputId": "02f21ffc-6919-4696-f61f-5d3893cf480b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pubmed (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xmltodict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Prd7RcxqQoHZ",
        "outputId": "6956ae71-adcc-41f0-bd8e-41a1c0e34c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Downloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: xmltodict\n",
            "Successfully installed xmltodict-0.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### By Langchain Retrievers"
      ],
      "metadata": {
        "id": "35KEU8wfGaB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.retrievers import ArxivRetriever\n",
        "from langchain_community.retrievers import WikipediaRetriever\n",
        "from langchain_community.retrievers import PubMedRetriever"
      ],
      "metadata": {
        "id": "enigRo89NF_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arxRetriever = ArxivRetriever(\n",
        "    load_max_docs=5,\n",
        "    get_ful_documents=True,\n",
        ")\n",
        "wikiRetriever = WikipediaRetriever()\n",
        "pubRetriever =  PubMedRetriever()"
      ],
      "metadata": {
        "id": "f_zR7qOaPN3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is RDMA?\""
      ],
      "metadata": {
        "id": "bZbtNOg1PUcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = arxRetriever.invoke(question)\n",
        "wikiDocs = wikiRetriever.invoke(question)\n",
        "pubMedDocs = pubRetriever.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvHn6gUPNRkd",
        "outputId": "8b0fb27c-0b22-4637-d761-049a7f6e99d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Too Many Requests, waiting for 0.20 seconds...\n",
            "Too Many Requests, waiting for 0.40 seconds...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs #page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "08pCB83rNVnk",
        "outputId": "754dccc4-7f1d-4d82-f81a-4217f98fee46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'Entry ID': 'http://arxiv.org/abs/1512.07007v1', 'Published': datetime.date(2015, 12, 22), 'Title': 'A statistical study of the subsurface structure and eruptivity of solar active regions', 'Authors': 'Chia-Hsien Lin'}, page_content='A statistical study of 77 solar active regions (ARs) is conducted to\\ninvestigate the existence of identifiable correlations between the subsurface\\nstructural disturbances and the activity level of the active regions. The\\ndisturbances examined in this study are $<|\\\\delta \\\\Gamma_1/\\\\Gamma_1|>$,\\n$<|\\\\delta c^2/c^2|>$, and $<|\\\\delta c^2/c^2-\\\\delta \\\\Gamma_1/\\\\Gamma_1|>$. where\\n$\\\\Gamma_1$ and $c$ are the thermodynamic properties of first adiabatic index\\nand sound speed modified by magnetic field, respectively. The averages are over\\nthree depth layers: $0.975-0.98 R_\\\\odot$, $0.98-0.99 R_\\\\odot$ and $0.99-0.995\\nR_\\\\odot$ to represent the structural disturbances in that layer. The level of\\nthe surface magnetic activity is measured by the Magnetic Activity Index (MAI)\\nof active region and the relative and absolute MAI differences (rdMAI and dMAI)\\nbetween the active and quiet regions. The eruptivity of each active region is\\nquantified by its Flare Index, total number of coronal mass ejections (CMEs),\\nand total kinetic energy of the CMEs. The existence and level of the\\ncorrelations are evaluated by scatter plots and correlation coefficients. No\\ndefinitive correlation can be claimed from the results. While a weak positive\\ntrend is visible between dMAI and $<|\\\\delta \\\\Gamma_1/\\\\Gamma_1|>$ and $<|\\\\delta\\nc^2/c^2|>$ in the layer $0.975-0.98 R_\\\\odot$, their correlation levels, being\\napproximately 0.6, are not sufficiently high to justify the correlation. Some\\nsubsurface disturbances are seen to increase with eruptivity indices among ARs\\nwith high eruptivity. The statistical significance of such trend, however,\\ncannot be ascertained due to the small number of very eruptive ARs in our\\nsample.'),\n",
              " Document(metadata={'Entry ID': 'http://arxiv.org/abs/1910.02158v2', 'Published': datetime.date(2019, 10, 14), 'Title': 'RDMA vs. RPC for Implementing Distributed Data Structures', 'Authors': 'Benjamin Brock, Yuxin Chen, Jiakun Yan, John D. Owens, Aydın Buluç, Katherine Yelick'}, page_content='Distributed data structures are key to implementing scalable applications for\\nscientific simulations and data analysis. In this paper we look at two\\nimplementation styles for distributed data structures: remote direct memory\\naccess (RDMA) and remote procedure call (RPC). We focus on operations that\\nrequire individual accesses to remote portions of a distributed data structure,\\ne.g., accessing a hash table bucket or distributed queue, rather than global\\noperations in which all processors collectively exchange information. We look\\nat the trade-offs between the two styles through microbenchmarks and a\\nperformance model that approximates the cost of each. The RDMA operations have\\ndirect hardware support in the network and therefore lower latency and\\noverhead, while the RPC operations are more expressive but higher cost and can\\nsuffer from lack of attentiveness from the remote side. We also run experiments\\nto compare the real-world performance of RDMA- and RPC-based data structure\\noperations with the predicted performance to evaluate the accuracy of our\\nmodel, and show that while the model does not always precisely predict running\\ntime, it allows us to choose the best implementation in the examples shown. We\\nbelieve this analysis will assist developers in designing data structures that\\nwill perform well on current network architectures, as well as network\\narchitects in providing better support for this class of distributed data\\nstructures.'),\n",
              " Document(metadata={'Entry ID': 'http://arxiv.org/abs/1805.06617v1', 'Published': datetime.date(2018, 5, 17), 'Title': 'What is \"fundamental\"?', 'Authors': 'Matt Visser'}, page_content='Our collective views regarding the question \"what is fundamental?\" are\\ncontinually evolving. These ontological shifts in what we regard as fundamental\\nare largely driven by theoretical advances (\"what can we calculate?\"), and\\nexperimental advances (\"what can we measure?\"). Rarely (in my view) is\\nepistemology the fundamental driver; more commonly epistemology reacts (after a\\nfew decades) to what is going on in the theoretical and experimental zeitgeist.')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wikiDocs # summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EEjuTsW0NW4a",
        "outputId": "cfe3037a-b978-40a1-b399-c24f93ca740a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'title': 'Remote direct memory access', 'summary': \"In computing, remote direct memory access (RDMA) is a direct memory access from the memory of one computer into that of another without involving either one's operating system. This permits high-throughput, low-latency networking, which is especially useful in massively parallel computer clusters.\", 'source': 'https://en.wikipedia.org/wiki/Remote_direct_memory_access'}, page_content='In computing, remote direct memory access (RDMA) is a direct memory access from the memory of one computer into that of another without involving either one\\'s operating system. This permits high-throughput, low-latency networking, which is especially useful in massively parallel computer clusters.\\n\\n\\n== Overview ==\\nRDMA supports zero-copy networking by enabling the network adapter to transfer data from the wire directly to application memory or from application memory directly to the wire, eliminating the need to copy data between application memory and the data buffers in the operating system. Such transfers require no work to be done by CPUs, caches, or context switches, and transfers continue in parallel with other system operations. This reduces latency in message transfer.\\nHowever, this strategy presents several problems related to the fact that the target node is not notified of the completion of the request (single-sided communications).\\n\\n\\n== Acceptance ==\\nAs of 2018 RDMA had achieved broader acceptance as a result of implementation enhancements that enable good performance over ordinary networking infrastructure. For example RDMA over Converged Ethernet (RoCE) now is able to run over either lossy or lossless infrastructure. In addition iWARP enables an Ethernet RDMA implementation at the physical layer using TCP/IP as the transport, combining the performance and latency advantages of RDMA with a low-cost, standards-based solution.  The RDMA Consortium and the DAT Collaborative have played key roles in the development of RDMA protocols and APIs for consideration by standards groups such as the Internet Engineering Task Force and the Interconnect Software Consortium.\\nHardware vendors have started working on higher-capacity RDMA-based network adapters, with rates of 100 Gbit/s reported. Software vendors, such as IBM, Red Hat and Oracle Corporation, support these APIs in their latest products, and as of 2013 engineers have started developing network adapters that implement RDMA over Ethernet. \\nBoth Red Hat Enterprise Linux and Red Hat Enterprise MRG have support for RDMA. Microsoft supports RDMA in Windows Server 2012 via SMB Direct. VMware ESXi also supports RDMA as of 2015.\\nCommon RDMA implementations include the Virtual Interface Architecture, RDMA over Converged Ethernet (RoCE), InfiniBand, Omni-Path and iWARP.\\n\\n\\n== Using RDMA ==\\nApplications access control structures using well-defined APIs originally designed for the InfiniBand Protocol (although the APIs can be used for any of the underlying RDMA implementations). Using send and completion queues, applications perform RDMA operations by submitting work queue entries (WQEs) into the submission queue (SQ) and getting notified of responses from the completion queue (CQ). \\n\\n\\n== Transport types ==\\nRDMA can transport data reliably or unreliably over the Reliably Connected (RC) and Unreliable Datagram (UD) transport protocols, respectively. The former has the benefit of preserving requests (no requests are lost), while the latter requires fewer queue pairs when handling multiple connections. This is due to the fact that UD is connection-less, allowing a single host to communicate with any other using a single queue.\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nRDMA Consortium\\nRFC 5040: A Remote Direct Memory Access Protocol Specification\\nA Tutorial of the RDMA Model\\n\"Why Compromise?\" // HPCwire, Gilad Shainer (Mellanox Technologies), 2006\\nA Critique of RDMA for high-performance computing\\nRDMA Reads: To Use or Not to Use?\\n[1]'),\n",
              " Document(metadata={'title': 'RDMA over Converged Ethernet', 'summary': 'RDMA over Converged Ethernet (RoCE) is a network protocol which allows remote direct memory access (RDMA) over an Ethernet network. There are multiple RoCE versions. RoCE v1 is an Ethernet link layer protocol and hence allows communication between any two hosts in the same Ethernet broadcast domain. RoCE v2 is an internet layer protocol which means that RoCE v2 packets can be routed. Although the RoCE protocol benefits from the characteristics of a converged Ethernet network, the protocol can also be used on a traditional or non-converged Ethernet network.', 'source': 'https://en.wikipedia.org/wiki/RDMA_over_Converged_Ethernet'}, page_content='RDMA over Converged Ethernet (RoCE) is a network protocol which allows remote direct memory access (RDMA) over an Ethernet network. There are multiple RoCE versions. RoCE v1 is an Ethernet link layer protocol and hence allows communication between any two hosts in the same Ethernet broadcast domain. RoCE v2 is an internet layer protocol which means that RoCE v2 packets can be routed. Although the RoCE protocol benefits from the characteristics of a converged Ethernet network, the protocol can also be used on a traditional or non-converged Ethernet network.\\n\\n\\n== Background ==\\nNetwork-intensive applications like networked storage or cluster computing need a network infrastructure with a high bandwidth and low latency. The advantages of RDMA over other network application programming interfaces such as Berkeley sockets are lower latency, lower CPU load and higher bandwidth. The RoCE protocol allows lower latencies than its predecessor, the iWARP protocol. There are RoCE HCAs (Host Channel Adapter) with a latency as low as 1.3 microseconds while the lowest known iWARP HCA latency in 2011 was 3 microseconds.\\n\\n\\n== RoCE v1 ==\\nThe RoCE v1 protocol is an Ethernet link layer protocol with Ethertype 0x8915. This means that the frame length limits of the Ethernet protocol apply: 1500 bytes for a regular Ethernet frame and 9000 bytes for a jumbo frame.\\n\\n\\n== RoCE v1.5 ==\\nThe RoCE v1.5 is an uncommon, experimental, non-standardized protocol that is based on the IP protocol. RoCE v1.5 uses the IP protocol field to differentiate its traffic from other IP protocols such as TCP and UDP. The value used for the protocol number is unspecified and is left to the deployment to select.\\n\\n\\n== RoCE v2 ==\\nThe RoCE v2 protocol exists on top of either the UDP/IPv4 or the UDP/IPv6 protocol. The UDP destination port number 4791 has been reserved for RoCE v2. Since RoCEv2 packets are routable the RoCE v2 protocol is sometimes called Routable RoCE or RRoCE. Although in general the delivery order of UDP packets is not guaranteed, the RoCEv2 specification requires that packets with the same UDP source port and the same destination address must not be reordered. In addition, RoCEv2 defines a congestion control mechanism that uses the IP ECN bits for marking and CNP frames for the acknowledgment notification. Software support for RoCE v2 is still emerging. Mellanox OFED 2.3 or later has RoCE v2 support and also Linux Kernel v4.5.\\n\\n\\n== RoCE versus InfiniBand ==\\nRoCE defines how to perform RDMA over Ethernet while the InfiniBand architecture specification defines how to perform RDMA over an InfiniBand network. RoCE was expected to bring InfiniBand applications, which are predominantly based on clusters, onto a common Ethernet converged fabric. Others expected that InfiniBand will keep offering a higher bandwidth and lower latency than what is possible over Ethernet.\\nThe technical differences between the RoCE and InfiniBand protocols are:\\n\\nLink Level Flow Control: InfiniBand uses a credit-based algorithm to guarantee lossless HCA-to-HCA communication. RoCE runs on top of Ethernet. Implementations may require lossless Ethernet network for reaching to performance characteristics similar to InfiniBand. Lossless Ethernet is typically configured via Ethernet flow control or priority flow control (PFC). Configuring a Data center bridging (DCB) Ethernet network can be more complex than configuring an InfiniBand network.\\nCongestion Control: Infiniband defines congestion control based on FECN/BECN marking, RoCEv2 defines a congestion control protocol that uses ECN  for marking as implemented in standard switches and CNP frames for acknowledgments.\\nInfiniBand switches typically have lower latency than Ethernet switches. Port-to-port latency for one particular type of Ethernet switch is 230 ns versus 100 ns for an InfiniBand switch with the same number of ports.\\n\\n\\n== RoCE versus iWARP ==\\nWhile the RoCE protocols define how to perform RDMA using Ethernet and UDP/IP  frames, the i'),\n",
              " Document(metadata={'title': 'SCSI RDMA Protocol', 'summary': 'In computing the SCSI RDMA Protocol (SRP) is a protocol that allows one computer to access SCSI devices attached to another computer via remote direct memory access (RDMA). The SRP protocol is also known as the SCSI Remote Protocol. The use of RDMA makes higher throughput and lower latency possible than what is generally available through e.g. the TCP/IP communication protocol.\\nThough the SRP protocol has been designed to use RDMA networks efficiently, it is also possible to implement the SRP protocol over networks that do not support RDMA.', 'source': 'https://en.wikipedia.org/wiki/SCSI_RDMA_Protocol'}, page_content='In computing the SCSI RDMA Protocol (SRP) is a protocol that allows one computer to access SCSI devices attached to another computer via remote direct memory access (RDMA). The SRP protocol is also known as the SCSI Remote Protocol. The use of RDMA makes higher throughput and lower latency possible than what is generally available through e.g. the TCP/IP communication protocol.\\nThough the SRP protocol has been designed to use RDMA networks efficiently, it is also possible to implement the SRP protocol over networks that do not support RDMA.\\n\\n\\n== History ==\\nSRP was published as an ANSI standard (ANSI INCITS 365-2002) in 2002 and renewed in 2007 and 2019.\\n\\n\\n== Related Protocols ==\\nAs with the ISCSI Extensions for RDMA (iSER) communication protocol, there is the notion of a target (a system that stores the data) and an initiator (a client accessing the target) with the target initiating data transfers. In other words, when an initiator writes data to a target, the target executes an RDMA read to fetch the data from the initiator and when a user issues a SCSI read command, the target sends an RDMA write to the initiator.\\nWhile the SRP protocol is easier to implement than the iSER protocol, iSER offers more management functionality, e.g. the target discovery infrastructure enabled by the iSCSI protocol.\\n\\n\\n== Performance ==\\nBandwidth and latency of storage targets supporting the SRP or the iSER protocol should be similar. On Linux, there are two SRP and two iSER storage target implementations available that run inside the kernel (SCST and LIO) and an iSER storage target implementation that runs in user space (STGT). Measurements have shown that the SCST SRP target has a lower latency and a higher bandwidth than the STGT iSER target. This is probably because the RDMA communication overhead is lower for a component implemented in the Linux kernel than for a user space Linux process, and not because of protocol differences.\\n\\n\\n== Implementations ==\\nIn order to use the SRP protocol, an SRP initiator implementation, an SRP target implementation and networking hardware supported by the initiator and target are needed. The following software SRP initiator implementations exist:\\n\\nLinux SRP initiator, available since November 2005 (kernel version 2.6.15).\\nWindows SRP initiator, available through the winOFED InfiniBand stack.\\nVMWare SRP initiator, available since January 2008 through Mellanox\\' OFED Drivers for VMware Infrastructure 3 and vSphere 4.\\nSolaris 10 SRP initiator, available through Sun\\'s download page.\\nSolaris 11 and OpenSolaris SRP initiator, integrated as a component of project COMSTAR.\\nThe IBM POWER virtual SCSI client driver for Linux (ibmvscsi), available since January 2008 (kernel version 2.6.24). Virtual SCSI allows client logical partitions to access I/O devices (disk, CD, and tape) that are owned by another logical partition.\\nThe following SRP target implementations exist:\\n\\nThe SCST SRP target implementation. This is a mature SRP target implementation available since 2008 via both SCST and OFED.\\nLinux LIO SRP target, available since January 2012 (kernel version 3.3), based on the SCST SRP target.\\nThe IBM POWER virtual SCSI target driver (ibmvstgt), available since January 2008 (kernel version 2.6.24).\\nDataDirect Network\\'s (DDN) disk subsystems such as the S2A9900 and SFA10000, which use the SRP target implementation in the disk subsystem\\'s controllers to present LUNs to servers (the servers act as SRP initiators).\\nIBM\\'s FlashSystem.\\nThe Solaris COMSTAR target, available since early 2009 in OpenSolaris and Solaris 11.\\n\\n\\n== See also ==\\niSCSI Extensions for RDMA (iSER)\\n\"SRP Update and Directions\". openfabrics.org. 27 Mar 2012.\\n\\n\\n== References ==')]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pubMedDocs # page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Zj-IIxWRPlfW",
        "outputId": "b511f50e-db1e-4d7b-ae4c-ceee6fe3779d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'uid': '39008394', 'Title': 'Visualization of Large Non-Trivially Partitioned Unstructured Data with Native Distribution on High-Performance Computing Systems.', 'Published': '2024-07-15', 'Copyright Information': ''}, page_content='Interactively visualizing large finite element simulation data on High-Performance Computing (HPC) systems poses several difficulties. Some of these relate to unstructured data, which, even on a single node, is much more expensive to render compared to structured volume data. Worse yet, in the data parallel rendering context, such data with highly non-convex spatial domain boundaries will cause rays along its silhouette to enter and leave a given rank\\'s domains at different distances. This straddling, in turn, poses challenges for both ray marching, which usually assumes successive elements to share a face, and compositing, which usually assumes a single fragment per pixel per rank. We holistically address these issues using a combination of three inter-operating techniques: first, we use a highly optimized GPU ray marching technique that, given an entry point, can march a ray to its exit point with highperformance by exploiting an exclusive-or (XOR) based compaction scheme. Second, we use hardware-accelerated ray tracing to efficiently find the proper entry points for these marching operations. Third, we use a \"deep\" compositing scheme to properly handle cases where different ranks\\' ray segments interleave in depth. We use GPU-to-GPU remote direct memory access (RDMA) to achieve interactive frame rates of 10-15 frames per second and higher for our motivating use case, the Fun3D NASA Mars Lander.'),\n",
              " Document(metadata={'uid': '38857910', 'Title': 'Prevalence and prognostic value of PD-L1 expression and tumor mutational burden in persistent, recurrent, or metastatic cervical cancer.', 'Published': '2024-05-23', 'Copyright Information': '© 2024. Asian Society of Gynecologic Oncology, Korean Society of Gynecologic Oncology, and Japan Society of Gynecologic Oncology.'}, page_content='OBJECTIVE: To evaluate the prevalence and prognostic role of programmed death ligand 1 (PD-L1) expression and tumor mutational burden (TMB) in patients with non-immunotherapy-treated advanced cervical cancer.\\nMETHODS: Clinical data were retrospectively collected from medical records between January 1, 2008, and December 31, 2016, at Asan Medical Center (Korea); archived tumor samples were assessed for PD-L1 expression (combined positive score [CPS] ≥1) and TMB (≥175 mutations/exome). Overall survival (OS) was defined as time from advanced diagnosis or initiation of first-line or second-line systemic therapy until death/last follow-up. The association of OS with PD-L1 expression and TMB were analyzed using the log-rank test and Cox proportional hazards model adjusted for covariates.\\nRESULTS: Of 267 patients, 76.0% had squamous cell carcinoma (SCC), 24.0% had adenocarcinoma (AC)/adenosquamous carcinoma (ASC), 64.4% had PD-L1 CPS ≥1, and 32.6% had TMB ≥175 mutations/exome. PD-L1 CPS ≥1 and TMB ≥175 mutations/exome were more prevalent in SCC than in AC/ASC (73.9% and 37.2% vs. 34.4% and 17.7%). There was no association between OS and PD-L1 expression (CPS ≥1 vs. <1: adjusted hazard ratio [HR]=1.14; 95% confidence interval [CI]=0.84-1.53 from advanced diagnosis); OS trended shorter for the subgroup with TMB ≥175 versus <175 mutations/exome (adjusted HR=1.29; 95% CI=0.95-1.75).\\nCONCLUSION: Retrospective analysis of non-immunotherapy-treated patients with advanced cervical cancer demonstrated a higher prevalence of PD-L1 CPS ≥1 and TMB ≥175 mutations/exome in SCC versus AC/ASC. PD-L1 CPS ≥1 was not associated with OS; TMB ≥175 mutations/exome showed a trend toward shorter OS. Additional studies are needed to confirm these findings.'),\n",
              " Document(metadata={'uid': '37331581', 'Title': 'Impact of lowering the US maximum contaminant level on arsenic exposure: Differences by race, region, and water arsenic in NHANES 2003-2014.', 'Published': '2023-06-16', 'Copyright Information': 'Copyright © 2023 Elsevier Ltd. All rights reserved.'}, page_content=\"Our objective was to evaluate regional and sociodemographic inequalities in water arsenic exposure reductions associated with the US Environmental Protection Agency's Final Arsenic Rule, which lowered the arsenic maximum contaminant level to 10\\xa0μg/L in public water systems. We analyzed 8544 participants from the 2003-14 National Health and Nutrition Examination Survey (NHANES) reliant on community water systems (CWSs). We estimated arsenic exposure from water by recalibrating urinary dimethylarsinate (rDMA) to remove smoking and dietary contributions. We evaluated mean differences and corresponding percent reductions of urinary rDMA comparing subsequent survey cycles to 2003-04 (baseline), stratified by region, race/ethnicity, educational attainment, and tertile of CWS arsenic assigned at the county level. The overall difference (percent reduction) in urine rDMA was 0.32\\xa0μg/L (9%) among participants with the highest tertile of CWS arsenic, comparing 2013-14 to 2003-04. Declines in urinary rDMA were largest in regions with the highest water arsenic: the South [0.57\\xa0μg/L (16%)] and West [0.46\\xa0μg/L, (14%)]. Declines in urinary rDMA levels were significant and largest among Mexican American [0.99\\xa0μg/L (26%)] and Non-Hispanic White [0.25\\xa0μg/L (10%)] participants. Reductions in rDMA following the Final Arsenic Rule were highest among participants with the highest CWS arsenic concentrations, supporting legislation can benefit those who need it the most, although additional efforts are still needed to address remaining inequalities in CWS arsenic exposure.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Web Scraping"
      ],
      "metadata": {
        "id": "ZJndGjV4GgBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium chromedriver-autoinstaller\n",
        "!apt-get update\n",
        "!apt-get install -y chromium-browser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S2bV0zGwH7UB",
        "outputId": "6d7e199f-a127-4501-9777-180cced0fff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.29.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting chromedriver-autoinstaller\n",
            "  Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.11/dist-packages (from chromedriver-autoinstaller) (24.2)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading selenium-4.29.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl (7.6 kB)\n",
            "Downloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, chromedriver-autoinstaller, trio, trio-websocket, selenium\n",
            "Successfully installed chromedriver-autoinstaller-0.6.4 outcome-1.3.0.post0 selenium-4.29.0 trio-0.29.0 trio-websocket-0.12.2 wsproto-1.2.0\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,321 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,664 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,708 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,235 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,830 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,688 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,649 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,957 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,533 kB]\n",
            "Fetched 29.0 MB in 3s (9,464 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor libfuse3-3 liblzo2-2 snapd squashfs-tools systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 snapd squashfs-tools systemd-hwe-hwdb udev\n",
            "0 upgraded, 8 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 30.1 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.66.1+22.04 [27.6 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 30.1 MB in 2s (14.0 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../0-apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../1-liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../2-squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Selecting previously unselected package udev.\n",
            "Preparing to unpack .../3-udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../4-libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../5-snapd_2.66.1+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.66.1+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.66.1+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 125384 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import chromedriver_autoinstaller\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from urllib.parse import quote, quote_plus\n",
        "import re"
      ],
      "metadata": {
        "id": "bUyHDI37H8y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_on_pubmedi(query):\n",
        "  url = \"https://pubmedisearch.com/share/\"\n",
        "  params = quote(query)\n",
        "  return url + params"
      ],
      "metadata": {
        "id": "n1R_ibmxXPaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Retriever"
      ],
      "metadata": {
        "id": "zQlDN9zJmiZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ7fADWrnPqn",
        "outputId": "bc12f5c9-817b-4bb7-ba47-1b31650b214f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain.schema import BaseRetriever, Document\n",
        "\n",
        "class CustomPubMedRetriever(BaseRetriever):\n",
        "    def __init__(self, top_k):\n",
        "        super().__init__()\n",
        "        self._pattern = re.compile(\"^https://pubmed.+/[0-9]{8}/$\")\n",
        "        self._top_k = top_k\n",
        "    def _is_pubmed_article(self, link):\n",
        "        return self._pattern.match(link)\n",
        "\n",
        "    def _search_on_pubmedi(self, query):\n",
        "        url = \"https://pubmedisearch.com/share/\"\n",
        "        params = quote(query)\n",
        "        return url + params\n",
        "\n",
        "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')  # Run Chrome in headless mode (no visible window) [1, 2, 3]\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--ignore-certificate-errors')\n",
        "        chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
        "\n",
        "\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "        url = self._search_on_pubmedi(query)\n",
        "        print(\"Searching for ...\", url)\n",
        "        driver.get(url)\n",
        "        try:\n",
        "          element = WebDriverWait(driver, 10).until(\n",
        "              EC.presence_of_element_located((By.XPATH, \"/html/body/div/div/div/button\"))\n",
        "          )\n",
        "          all_links = driver.find_elements(By.TAG_NAME, \"a\")\n",
        "          gather_results = set()\n",
        "          papers = []\n",
        "          for link in all_links:\n",
        "            href = link.get_attribute(\"href\")\n",
        "            if self._is_pubmed_article(href):\n",
        "              gather_results.add(href)\n",
        "              if len(gather_results) == self._top_k: # Find a way to change this\n",
        "                break\n",
        "          #print(\"Links gathered \", gather_results)\n",
        "          for link in list(gather_results):\n",
        "            driver.get(link)\n",
        "            paper = Document(\n",
        "    page_content=driver.find_element(By.XPATH, \"//div[@id='eng-abstract']/p\").text,\n",
        "    metadata={\"link\": link, \"title\": driver.title})\n",
        "            papers.append(paper)\n",
        "          driver.quit()\n",
        "          return papers\n",
        "        except Exception as e:\n",
        "          print(e)\n",
        "          driver.quit()\n",
        "          return []\n",
        "\n",
        "\n",
        "class CustomArxivRetriever(BaseRetriever):\n",
        "    def __init__(self, top_k):\n",
        "        super().__init__()\n",
        "        self._top_k = top_k\n",
        "\n",
        "    def _search_on_arxiv(query):\n",
        "      url = \"https://arxivxplorer.com/?query=\"\n",
        "      params = quote_plus(query)\n",
        "      return url + params\n",
        "\n",
        "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "      chrome_options = Options()\n",
        "      chrome_options.add_argument('--headless')  # Run Chrome in headless mode (no visible window) [1, 2, 3]\n",
        "      chrome_options.add_argument('--no-sandbox')\n",
        "      chrome_options.add_argument('--ignore-certificate-errors')\n",
        "      chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
        "\n",
        "\n",
        "      driver = webdriver.Chrome(options=chrome_options)\n",
        "      url = self._search_on_arxiv(query)\n",
        "      print(\"Searching for ...\", url)\n",
        "      driver.get(url)\n",
        "      try:\n",
        "        element = WebDriverWait(driver, 10).until(\n",
        "            EC.presence_of_all_elements_located((By.XPATH, \"//div[contains(@class, 'css-1086mds')]/div/div/div/p\"))\n",
        "        )\n",
        "        papers = []\n",
        "        result_divs = driver.find_elements(By.XPATH, \"//div[contains(@class, 'css-1086mds')]\")\n",
        "        for div in result_divs:\n",
        "          div = div.find_element(By.XPATH, \"./div[2]/div/div/p\")\n",
        "          papers.append(div.text)\n",
        "        driver.quit()\n",
        "        return papers[:self.top_k]\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        driver.quit()\n",
        "        return []\n",
        "docs = []\n",
        "pm_retriever = CustomPubMedRetriever(top_k=10)\n",
        "arx_retriever = CustomArxivRetriever(top_k=10)"
      ],
      "metadata": {
        "id": "JFnSohjMr5zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Engineering"
      ],
      "metadata": {
        "id": "XYWPRV2HGkb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts.prompt import PromptTemplate"
      ],
      "metadata": {
        "id": "RrumS03qQuzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\", \"context\"], template=\"\"\"\n",
        "      You are an assistant thats answers questions and settles arguments with facts that you infer from the context provided below.\n",
        "      When the questions are inherently trying to prove a stance, ensure that you find evidence for both 'for' and 'against' stances, to ensure that you are not biased towards one stance.\n",
        "      Your job is explain why and also provide support for why one of the two stances is closer to the truth or is actually true\n",
        "      For each paper you read, make sure you cite the authors and provide links to it, and what parts of your answer are from it.\n",
        "\n",
        "      You have to make sure that you do not hallucinate responses, say it when you cant find enough evidence.\n",
        "      You need to include background information for some medical jargon, answer for a college freshman.\n",
        "      You can use other information from the web or outside the context, to explain some topics in the answer you will find from the context.\n",
        "      DO NOT make up facts. DO NOT answer from outside the context, only go out of the context to explain jargon used in it.\n",
        "\n",
        "      Question: {question}\n",
        "      Context: {context}\n",
        "     \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "iHoFUdDSa4Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "K8MnaYA4cGrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain Building"
      ],
      "metadata": {
        "id": "1rFgW_KDml3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "rag_chain = (\n",
        "    {\"context\": pm_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "Qz-gnOSjcaOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in rag_chain.stream(\"is brocolli is bad for hypothyroidism in humans?\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gabHY9kmlyFj",
        "outputId": "a89cf433-23aa-4d4c-ea12-c3990a7979e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for ... https://pubmedisearch.com/share/is%20brocolli%20is%20bad%20for%20hypothyroidism%20in%20humans%3F\n",
            "Based on the information provided in the context, broccoli and broccoli sprouts have been studied extensively in relation to their effects on thyroid function. While there are concerns about the goitrogenic effects of compounds found in broccoli, such as sulforaphane and glucosinolates, the evidence suggests that broccoli consumption does not have adverse effects on thyroid function in humans.\n",
            "\n",
            "A study conducted by Krajka-Kuzniak et al. (2020) investigated the impact of broccoli sprouts on thyroid function in rats under conditions of hypothyroidism induced by iodine deficiency or sulfadimethoxine ingestion. The study found that the levels of thyroid-stimulating hormone (TSH), free triiodothyronine (fT3), and free thyroxine (fT4) remained unchanged following broccoli sprouts ingestion. In fact, broccoli sprouts were found to have a protective effect against sulfadimethoxine-induced thyroid damage. Additionally, the study observed an increase in the activity of thioredoxin reductase (TR) in the thyroid in response to sprouts ingestion, indicating a beneficial influence on the antioxidant balance of the thyroid gland. The study concluded that broccoli sprouts did not have negative effects on thyroid function and may even offer protective benefits under certain conditions.\n",
            "\n",
            "Furthermore, a systematic review conducted by Kmieć et al. (2020) assessed the impact of dietary interventions, including brassica vegetables like broccoli, on thyroid function. The review analyzed a wide range of studies and found that the majority of results did not support previous assumptions that brassica plants have antithyroid effects in humans. Instead, including brassica vegetables in the diet, especially when accompanied by adequate iodine intake, was not associated with adverse effects on thyroid function.\n",
            "\n",
            "In conclusion, based on the available evidence from these studies, it can be inferred that broccoli consumption is not bad for hypothyroidism in humans. In fact, broccoli and broccoli sprouts may have protective effects on thyroid function and antioxidant balance, particularly when consumed as part of a balanced diet. It is important to note that individual responses to foods may vary, and individuals with specific thyroid conditions should consult with healthcare professionals for personalized dietary recommendations. \n",
            "\n",
            "References:\n",
            "1. Krajka-Kuzniak, V., Paluszczak, J., & Baer-Dubowska, W. (2020). The effect of broccoli sprouts on thyroid function in rats with iodine deficiency or sulfadimethoxine ingestion. Nutrients, 12(2), 1-16. [https://www.mdpi.com/2072-6643/12/2/456]\n",
            "2. Kmieć, P., Swiderska, E., & Krajka-Kuzniak, V. (2020). Impact of dietary interventions on thyroid function: A systematic review. Nutrients, 12(6), 1-18. [https://www.mdpi.com/2072-6643/12/6/1769]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_CSE_ID\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlyeW7_GlfuZ",
        "outputId": "caf8ad59-8320-49b0-dd29-f8077394fe82"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8LBdOHVlgLe",
        "outputId": "143dfc73-099f-4497-fbab-e0a4a89e037e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangGraph"
      ],
      "metadata": {
        "id": "0GIxYuhJDwk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langgraph langsmith"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UcFyqJHlEgFv",
        "outputId": "057458e2-bff2-4049-d257-7c68f0ea590a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.2-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.40)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.16-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.53-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.15)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (4.12.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (2.3.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.1)\n",
            "Downloading langgraph-0.3.2-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.9/130.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.16-py3-none-any.whl (38 kB)\n",
            "Downloading langgraph_prebuilt-0.1.1-py3-none-any.whl (24 kB)\n",
            "Downloading langgraph_sdk-0.1.53-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.3.2 langgraph-checkpoint-2.0.16 langgraph-prebuilt-0.1.1 langgraph-sdk-0.1.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_react_agent, load_tools\n",
        "from typing import Optional, Type\n",
        "from langchain.tools import StructuredTool, tool"
      ],
      "metadata": {
        "id": "PJf1WjSCDykE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"tools\", \"agent_scratchpad\", \"tool_names\"], template=\"\"\"\n",
        "Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}], and nothing else.\n",
        "Action Input: the input to the action, a query string in this case or it can be the value passed to the action function as an argument\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "For example, if you need to search for research papers on compiler optimizations using machine learning, you should output:\n",
        "\n",
        "Action: arxiv-search\n",
        "Action Input: \"machine learning compiler optimization performance\"\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "f--pamUMLyS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Arxiv Tool"
      ],
      "metadata": {
        "id": "DwbA6lQIfJ-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_on_arxiv(query):\n",
        "  url = \"https://arxivxplorer.com/?query=\"\n",
        "  params = quote_plus(query)\n",
        "  return url + params\n",
        "\n",
        "def retrieve_arxiv_articles(query, max_results=10):\n",
        "  chrome_options = Options()\n",
        "  chrome_options.add_argument('--headless')  # Run Chrome in headless mode (no visible window) [1, 2, 3]\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('--ignore-certificate-errors')\n",
        "  chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
        "\n",
        "\n",
        "  driver = webdriver.Chrome(options=chrome_options)\n",
        "  url = search_on_arxiv(query)\n",
        "  #print(\"Searching for ...\", url)\n",
        "  driver.get(url)\n",
        "  try:\n",
        "    element = WebDriverWait(driver, 10).until(\n",
        "        EC.presence_of_all_elements_located((By.XPATH, \"//div[contains(@class, 'css-1086mds')]/div/div/div/p\"))\n",
        "    )\n",
        "    papers = []\n",
        "    result_divs = driver.find_elements(By.XPATH, \"//div[contains(@class, 'css-1086mds')]\")\n",
        "    for div in result_divs:\n",
        "      div = div.find_element(By.XPATH, \"./div[2]/div/div/p\")\n",
        "      papers.append(div.text)\n",
        "    driver.quit()\n",
        "    return papers[:10]\n",
        "  except Exception as e:\n",
        "    print(e, \"error from arxiv\")\n",
        "    driver.quit()\n",
        "    return []"
      ],
      "metadata": {
        "id": "RhmBxf0CmSeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pubmed Tool"
      ],
      "metadata": {
        "id": "CzaFPY9ofMDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_link_pubmed_article(link):\n",
        "  pattern = re.compile(\"^https://pubmed.+/[0-9]{8}/$\")\n",
        "  return pattern.match(link)\n",
        "\n",
        "def search_on_pubmedi(query):\n",
        "  url = \"https://pubmedisearch.com/share/\"\n",
        "  params = quote(query)\n",
        "  return url + params\n",
        "\n",
        "def retrieve_pubmed_articles(query, max_results=10):\n",
        "  chrome_options = Options()\n",
        "  chrome_options.add_argument('--headless')  # Run Chrome in headless mode (no visible window) [1, 2, 3]\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('gnore-certificate-errors')\n",
        "  chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
        "\n",
        "\n",
        "  driver = webdriver.Chrome(options=chrome_options)\n",
        "  url = search_on_pubmedi(query)\n",
        "  #print(\"Searching for ...\", url)\n",
        "  driver.get(url)\n",
        "  try:\n",
        "    element = WebDriverWait(driver, 10).until(\n",
        "        EC.presence_of_element_located((By.XPATH, \"/html/body/div/div/div/button\"))\n",
        "    )\n",
        "    all_links = driver.find_elements(By.TAG_NAME, \"a\")\n",
        "    gather_results = set()\n",
        "    papers = [] # (abstract, link, title)\n",
        "    for link in all_links:\n",
        "      href = link.get_attribute(\"href\")\n",
        "      if is_link_pubmed_article(href):\n",
        "        gather_results.add(href)\n",
        "        if len(gather_results) == max_results:\n",
        "          break\n",
        "    for link in list(gather_results):\n",
        "      driver.get(link)\n",
        "      paper = (driver.find_element(By.XPATH, \"//div[@id='eng-abstract']/p\").text, link, driver.title)\n",
        "      papers.append(paper)\n",
        "    driver.quit()\n",
        "    return papers\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    driver.quit()\n",
        "    return []\n"
      ],
      "metadata": {
        "id": "APYP4727YUmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pubmed_search(query):\n",
        "  articles = \"\\n\\n\".join([p[0] for p in retrieve_pubmed_articles(query)])\n",
        "  llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0.5)\n",
        "  prompt_template = PromptTemplate( input_variables=[\"question\", \"context\"], template=\"\"\"\n",
        "    You are an assistant thats answers questions and settles arguments with facts that you infer from the context provided below.\n",
        "    When the questions are inherently trying to prove a stance, ensure that you find evidence for both 'for' and 'against' stances, to ensure that you are not biased towards one stance.\n",
        "    Your job is explain why and also provide support for why one of the two stances is closer to the truth or is actually true\n",
        "    For each paper you read, make sure you cite the authors and provide links to it, and what parts of your answer are from it.\n",
        "\n",
        "    You have to make sure that you do not hallucinate responses, say it when you cant find enough evidence.\n",
        "    You need to include background information for some medical jargon, answer for a college freshman.\n",
        "    You can use other information from the web or outside the context, to explain some topics in the answer you will find from the context.\n",
        "    DO NOT make up facts. DO NOT answer from outside the context, only go out of the context to explain jargon used in it.\n",
        "\n",
        "    Question: {question}\n",
        "    Context: {context}\n",
        "  \"\"\"\n",
        "  )\n",
        "  prompt = prompt_template.format(question=query, context=articles)\n",
        "  return llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "Ar5hXvRibbb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def arxiv_search(query):\n",
        "  articles = \"\\n\\n\".join(retrieve_arxiv_articles(query))\n",
        "  llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0.5, max_tokens=3000)\n",
        "  prompt_template = PromptTemplate( input_variables=[\"question\", \"context\"], template=\"\"\"\n",
        "    You are an assistant thats answers questions and settles arguments with facts that you infer from the context provided below.\n",
        "    When the questions are inherently trying to prove a stance, ensure that you find evidence for both 'for' and 'against' stances, to ensure that you are not biased towards one stance.\n",
        "    Your job is explain why and also provide support for why one of the two stances is closer to the truth or is actually true\n",
        "    For each paper you read, make sure you cite the authors and provide links to it, and what parts of your answer are from it.\n",
        "\n",
        "    You have to make sure that you do not hallucinate responses, say it when you cant find enough evidence.\n",
        "    You need to include background information for some medical jargon, answer for a college freshman.\n",
        "    You can use other information from the web or outside the context, to explain some topics in the answer you will find from the context.\n",
        "    DO NOT make up facts. DO NOT answer from outside the context, only go out of the context to explain jargon used in it.\n",
        "\n",
        "    Question: {question}\n",
        "    Context: {context}\n",
        "  \"\"\"\n",
        "  )\n",
        "  prompt = prompt_template.format(question=query, context=articles)\n",
        "  return llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "Kip3iZ1FgI0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pm_search = StructuredTool.from_function(\n",
        "    func=pubmed_search,\n",
        "    name=\"pubmed-search\",\n",
        "    description=\"useful for when you need to answer questions about medical scientific facts, claims, or data\",\n",
        ")"
      ],
      "metadata": {
        "id": "tpVscTTqbZrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arx_search = StructuredTool.from_function(\n",
        "    func=arxiv_search,\n",
        "    name=\"arxiv-search\",\n",
        "    description=\"useful for when you need to answer questions about computer related scientific facts, claims, or data\",\n",
        ")"
      ],
      "metadata": {
        "id": "B252HXVRgEDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.utilities import GoogleSearchAPIWrapper\n",
        "from langchain.utilities import TextRequestsWrapper\n",
        "from langchain.agents import Tool\n",
        "GOOGLE_CSE_ID = os.getenv('GOOGLE_CSE_ID')\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "google_search = GoogleSearchAPIWrapper(google_api_key=GOOGLE_API_KEY, google_cse_id=GOOGLE_CSE_ID)\n",
        "\n",
        "requests = TextRequestsWrapper()"
      ],
      "metadata": {
        "id": "j13Jien5lSO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    arx_search,\n",
        "    pm_search,\n",
        "    Tool(\n",
        "        name = \"Google-Search\",\n",
        "        func=google_search.run,\n",
        "        description=\"useful for when you need to search google for some answers not fully available on literature sites, Might want to search this to compliment other searches\"\n",
        "    ),\n",
        "    # Tool(\n",
        "    #     name = \"Intermediate-Answer\",\n",
        "    #     func=requests.get,\n",
        "    #     description=\"useful for when you need to make an intermediate answer to the question based on the context you have gathered so far. this might help you to decide which tool to use next\"\n",
        "    # ),\n",
        "\n",
        "]\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "agent = create_react_agent(llm, tools, agent_prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True, verbose=True)"
      ],
      "metadata": {
        "id": "5dDsSVTpMboT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent_executor.invoke({\"input\":\"How does COVID cause deaths and what are its symptoms in detail? Also tell me number of deaths caused by major symptoms\",\n",
        "                                  \"tool_names\":[\"Google-Search\", \"pubmed-search\", \"arxiv-search\"]})\n",
        "response['output']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "StspeJdBcP_L",
        "outputId": "f46ca812-39dd-43f5-c8ea-c0440a67f836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo answer this question, I need to gather information on how COVID-19 causes deaths, its detailed symptoms, and the number of deaths caused by major symptoms. I will start by searching for detailed information on COVID-19 symptoms and how it leads to fatalities using PubMed, as it is a medical-related query. Then, I will look for statistics on the number of deaths caused by major symptoms using Google Search.\n",
            "\n",
            "Action: pubmed-search\n",
            "Action Input: \"COVID-19 symptoms and causes of death\"\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mcontent='Based on the information provided in the context, as well as additional background information, it can be inferred that the symptoms of COVID-19 in adults can vary widely, with some common symptoms being cough, fever, and respiratory distress. However, extrapulmonary symptoms are also common, and loss of sense of smell may be an early sign of the disease. The mortality rate for COVID-19 patients can vary, with risk factors for severe disease and death including comorbid conditions such as hypertension, cardiovascular disease, diabetes mellitus, and chronic obstructive pulmonary disease. Other factors such as older age, obesity, end-stage renal disease, and a higher neutrophil-lymphocyte ratio have also been implicated.\\n\\nIn terms of causes of death related to COVID-19, several studies have been conducted to analyze this. One study conducted autopsies on 26 patients who died after SARS-CoV-2 infection and found that septic shock and multi-organ failure were common immediate causes of death, often due to suppurative pulmonary infection. Another study analyzed 92 death cases with COVID-19 and found that complications such as acute respiratory distress syndrome (ARDS), myocardial injury, liver injury, renal insufficiency, and multiple organ dysfunction syndrome (MODS) were common among deceased patients.\\n\\nBased on the evidence provided, it can be inferred that the majority of patients who died after SARS-CoV-2 infection and COVID-19 had direct causes of death related to COVID-19 itself, with contributory implications of preexisting health conditions. This suggests that COVID-19 played a significant role in the deaths of these patients.\\n\\nReferences:\\n1. Autopsy findings and causes of death in patients with COVID-19: A review of 26 cases - https://www.sciencedirect.com/science/article/pii/S2213260020301836\\n2. Clinical characteristics and complications in death cases with COVID-19: A retrospective study - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7097235/' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 412, 'prompt_tokens': 1434, 'total_tokens': 1846, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-c240ed78-e3a2-4b63-8de1-4d98caf33bd9-0'\u001b[0m\u001b[32;1m\u001b[1;3mThe information from PubMed provides a detailed overview of COVID-19 symptoms and causes of death. COVID-19 symptoms can vary widely, with common symptoms including cough, fever, and respiratory distress. Extrapulmonary symptoms, such as loss of smell, are also common. The risk factors for severe disease and death include comorbid conditions like hypertension, cardiovascular disease, diabetes, and chronic obstructive pulmonary disease, as well as factors like older age and obesity.\n",
            "\n",
            "The causes of death related to COVID-19 often involve septic shock and multi-organ failure, frequently due to suppurative pulmonary infection. Complications such as acute respiratory distress syndrome (ARDS), myocardial injury, liver injury, renal insufficiency, and multiple organ dysfunction syndrome (MODS) are common among deceased patients.\n",
            "\n",
            "Now, I need to find statistics on the number of deaths caused by major symptoms of COVID-19. I will use Google Search for this information.\n",
            "\n",
            "Action: Google-Search\n",
            "Action Input: \"number of deaths caused by major COVID-19 symptoms\"\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mSep 20, 2024 ... COVID-19, also called coronavirus disease 2019, is an illness caused by a virus. The virus is called severe acute respiratory syndrome coronavirus 2, or more ... Jan 6, 2025 ... The number of deaths among people over age 65 is 97 times higher ... from COVID-19 and severe illness. For treatment options. You must ... Jan 1, 2021 ... The COVID-19 death toll surpasses that of the severe acute respiratory syndrome (SARS) outbreak from 17 years ago, totaling 908 reported deaths ... Jan 31, 2025 ... Severe allergic reactions following vaccination are rare but can be life threatening. Signs and symptoms of a severe allergic reaction can ... Oct 29, 2024 ... ... coronavirus disease (COVID-19). It was also the leading killer of people with HIV and a major cause of deaths related to antimicrobial ... COVID-19 can be severe, and has caused millions of deaths ... Diagnosis by examination alone is difficult since many COVID-19 signs and symptoms can be caused by ... Feb 19, 2021 ... COVID-19 is caused by infection with severe acute respiratory syndrome coronavirus ... While raw numbers of deaths suggest overall COVID-19 ... Aug 9, 2023 ... People may experience different symptoms from COVID-19. Symptoms ... The consequences of severe COVID-19 include death, respiratory ... Number of deaths, 2.2 million, 50 million. Highest risk population, 65+ with comorbidities, 25–40 year olds. Cause of death, Overactive immune system leading to ... Since the number of actual infections and deaths from COVID-19 is not known, one has to be careful in interpreting the CFR. What is important to note about ...\u001b[0m\u001b[32;1m\u001b[1;3mThe Google search did not provide specific statistics on the number of deaths caused by major symptoms of COVID-19. However, it is known that COVID-19 has caused millions of deaths worldwide, with severe cases often leading to complications such as acute respiratory distress syndrome (ARDS), multi-organ failure, and other critical conditions that contribute to mortality.\n",
            "\n",
            "Final Answer: COVID-19 symptoms include cough, fever, respiratory distress, and loss of smell. Severe cases can lead to complications like ARDS, myocardial injury, liver injury, renal insufficiency, and multi-organ dysfunction syndrome (MODS), often resulting in death. Risk factors for severe disease and death include comorbid conditions like hypertension, cardiovascular disease, diabetes, and chronic obstructive pulmonary disease, as well as older age and obesity. While specific statistics on deaths caused by major symptoms are not readily available, COVID-19 has caused millions of deaths globally, primarily due to severe respiratory and systemic complications.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'COVID-19 symptoms include cough, fever, respiratory distress, and loss of smell. Severe cases can lead to complications like ARDS, myocardial injury, liver injury, renal insufficiency, and multi-organ dysfunction syndrome (MODS), often resulting in death. Risk factors for severe disease and death include comorbid conditions like hypertension, cardiovascular disease, diabetes, and chronic obstructive pulmonary disease, as well as older age and obesity. While specific statistics on deaths caused by major symptoms are not readily available, COVID-19 has caused millions of deaths globally, primarily due to severe respiratory and systemic complications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 340
        }
      ]
    }
  ]
}